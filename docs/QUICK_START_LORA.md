# HuggingFace + LoRA å¿«é€Ÿå¼€å§‹

## ğŸš€ ä¸€åˆ†é’Ÿä¸Šæ‰‹

### è®­ç»ƒ

```bash
torchrun --nproc_per_node 8 trainer/train_hf_lora.py \
  --base_model Qwen/Qwen2-0.5B-Instruct \
  --data_path dataset/pretrain_hq_split.jsonl \
  --lora_rank 8 \
  --batch_size 4 \
  --batch_by_samples \
  --max_branches_per_sample 16 \
  --min_branches_per_sample 1 \
  --rope_2d_ratio 0.5 \
  --epochs 3 \
  --ddp
```

### æ¨ç†

```bash
# äº¤äº’å¼å¯¹è¯
python scripts/inference_hf_lora.py \
  --base_model Qwen/Qwen2-0.5B-Instruct \
  --lora_path out/lora/hf_lora_hf_final.pth \
  --lora_rank 8 \
  --rope_2d_ratio 0.5 \
  --mode chat

# å•æ¬¡ç”Ÿæˆ
python scripts/inference_hf_lora.py \
  --base_model Qwen/Qwen2-0.5B-Instruct \
  --lora_path out/lora/hf_lora_hf_final.pth \
  --lora_rank 8 \
  --mode generate \
  --prompt "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
```

## ğŸ“‹ æ ¸å¿ƒå‚æ•°é€ŸæŸ¥

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--base_model` | - | HF æ¨¡å‹è·¯å¾„ | Qwen/Qwen2-xxx |
| `--lora_rank` | 8 | LoRA ç§© | å°æ¨¡å‹:8, ä¸­æ¨¡å‹:16, å¤§æ¨¡å‹:32 |
| `--rope_2d_ratio` | 0.5 | Branch ç»´åº¦é¢‘ç‡æ¯”ä¾‹ | 0.3-0.7 |
| `--batch_size` | 16 | æ‰¹æ¬¡å¤§å° | æ ¹æ®æ˜¾å­˜è°ƒæ•´ |
| `--batch_by_samples` | False | æŒ‰æ ·æœ¬æ•°è®¡æ•° | å»ºè®®å¯ç”¨ |
| `--max_branches_per_sample` | None | æœ€å¤§åˆ†æ”¯æ•° | 8-16 |
| `--min_branches_per_sample` | 1 | æœ€å°åˆ†æ”¯æ•° | 1-4 |
| `--epochs` | 3 | è®­ç»ƒè½®æ•° | 1-5 |

### æ¨ç†å‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ | æ¨èå€¼ |
|------|--------|------|--------|
| `--lora_rank` | 8 | **å¿…é¡»ä¸è®­ç»ƒä¸€è‡´** | åŒè®­ç»ƒ |
| `--rope_2d_ratio` | 0.5 | **å¿…é¡»ä¸è®­ç»ƒä¸€è‡´** | åŒè®­ç»ƒ |
| `--mode` | chat | chat/generate | chat |
| `--temperature` | 0.7 | æ¸©åº¦ | äº‹å®é—®ç­”:0.2, åˆ›æ„å†™ä½œ:0.8 |
| `--max_new_tokens` | 512 | æœ€å¤§ç”Ÿæˆé•¿åº¦ | 100-2048 |

## âš ï¸ é‡è¦æç¤º

### âœ… å·²è‡ªåŠ¨å¤„ç†ï¼ˆæ— éœ€å…³å¿ƒï¼‰

- âœ… **pos2d è‡ªåŠ¨æ³¨å…¥**ï¼šæ¨ç†è„šæœ¬å·²è‡ªåŠ¨å¤„ç† 2D RoPE çš„ pos2d
- âœ… **prepare_inputs_for_generation é‡å†™**ï¼šè‡ªåŠ¨åœ¨æ¯æ¬¡ç”Ÿæˆå‰è°ƒç”¨ `set_rope_pos2d`
- âœ… **å¢é‡ç”Ÿæˆ**ï¼šæ¯æ­¥è‡ªåŠ¨æ›´æ–° pos2d

### âš ï¸ å¿…é¡»æ³¨æ„

1. **å‚æ•°ä¸€è‡´æ€§**ï¼š
   - `--lora_rank` è®­ç»ƒå’Œæ¨ç†å¿…é¡»ä¸€è‡´
   - `--rope_2d_ratio` è®­ç»ƒå’Œæ¨ç†å¿…é¡»ä¸€è‡´
   - è®­ç»ƒç”¨äº† `--patch_rope`ï¼Œæ¨ç†ä¹Ÿå¿…é¡»ç”¨ï¼ˆé»˜è®¤å¯ç”¨ï¼‰

2. **æ•°æ®æ ¼å¼**ï¼š
   - è®­ç»ƒæ•°æ®å¿…é¡»æ˜¯ Parallel æ ¼å¼ï¼ˆ`main` + `branches`ï¼‰
   - ä¸æ”¯æŒæ ‡å‡† SFT æ ¼å¼

3. **æ˜¾å­˜ä¼˜åŒ–**ï¼š
   - æ˜¾å­˜ä¸è¶³æ—¶å‡å° `--batch_size`
   - ä½¿ç”¨ `--accumulation_steps` è¡¥å¿
   - ä½¿ç”¨ `--dtype bfloat16`

## ğŸ“Š ä¸åŒæ¨¡å‹é…ç½®

### Qwen2-0.5Bï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰

```bash
# è®­ç»ƒ
torchrun --nproc_per_node 4 trainer/train_hf_lora.py \
  --base_model Qwen/Qwen2-0.5B-Instruct \
  --lora_rank 8 \
  --batch_size 8 \
  --batch_by_samples \
  --max_branches_per_sample 16 \
  --min_branches_per_sample 1

# æ¨ç†
python scripts/inference_hf_lora.py \
  --base_model Qwen/Qwen2-0.5B-Instruct \
  --lora_path out/lora/hf_lora_hf_final.pth \
  --lora_rank 8 \
  --mode chat
```

### Qwen2.5-1.5Bï¼ˆç”Ÿäº§æ¨èï¼‰

```bash
# è®­ç»ƒ
torchrun --nproc_per_node 8 trainer/train_hf_lora.py \
  --base_model Qwen/Qwen2.5-1.5B-Instruct \
  --lora_rank 16 \
  --batch_size 4 \
  --accumulation_steps 2 \
  --batch_by_samples \
  --max_branches_per_sample 12 \
  --min_branches_per_sample 2 \
  --rope_2d_ratio 0.5

# æ¨ç†
python scripts/inference_hf_lora.py \
  --base_model Qwen/Qwen2.5-1.5B-Instruct \
  --lora_path out/lora/hf_lora_hf_final.pth \
  --lora_rank 16 \
  --rope_2d_ratio 0.5 \
  --mode chat
```

### Qwen2.5-7Bï¼ˆå¤§æ¨¡å‹ï¼‰

```bash
# è®­ç»ƒï¼ˆéœ€è¦å¤§æ˜¾å­˜ï¼‰
torchrun --nproc_per_node 8 trainer/train_hf_lora.py \
  --base_model Qwen/Qwen2.5-7B-Instruct \
  --lora_rank 32 \
  --batch_size 1 \
  --accumulation_steps 8 \
  --batch_by_samples \
  --max_branches_per_sample 8 \
  --min_branches_per_sample 2 \
  --rope_2d_ratio 0.5 \
  --dtype bfloat16

# æ¨ç†
python scripts/inference_hf_lora.py \
  --base_model Qwen/Qwen2.5-7B-Instruct \
  --lora_path out/lora/hf_lora_hf_final.pth \
  --lora_rank 32 \
  --rope_2d_ratio 0.5 \
  --mode chat \
  --dtype bfloat16
```

## ğŸ”§ å¸¸è§é—®é¢˜å¿«é€Ÿä¿®å¤

### Q1: `RuntimeError: extra_pos2d is not set`

**åŸå› **ï¼šä½¿ç”¨äº†æ—§ç‰ˆæ¨ç†ä»£ç æˆ–è‡ªå·±å†™çš„ä»£ç æ²¡æœ‰è®¾ç½® pos2d

**è§£å†³**ï¼š
- âœ… ä½¿ç”¨ `scripts/inference_hf_lora.py`ï¼ˆå·²è‡ªåŠ¨å¤„ç†ï¼‰
- âœ… æˆ–å‚è€ƒ [INFERENCE_GUIDE.md](INFERENCE_GUIDE.md) æ‰‹åŠ¨å®ç°

### Q2: `size mismatch for ...`

**åŸå› **ï¼š`lora_rank` ä¸ä¸€è‡´

**è§£å†³**ï¼šç¡®ä¿æ¨ç†æ—¶çš„ `--lora_rank` ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´

### Q3: æ˜¾å­˜æº¢å‡º `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼ˆæŒ‰é¡ºåºå°è¯•ï¼‰ï¼š
1. å‡å° `--batch_size`ï¼ˆä» 8 â†’ 4 â†’ 2 â†’ 1ï¼‰
2. å¢åŠ  `--accumulation_steps`ï¼ˆ1 â†’ 2 â†’ 4ï¼‰
3. å‡å° `--max_branches_per_sample`ï¼ˆ16 â†’ 12 â†’ 8ï¼‰
4. ä½¿ç”¨ `--dtype bfloat16`

### Q4: ç”Ÿæˆç»“æœè´¨é‡å·®

**è°ƒæ•´ç”Ÿæˆå‚æ•°**ï¼š
```bash
# äº‹å®é—®ç­”ï¼ˆæ›´ç¡®å®šï¼‰
--temperature 0.2 --top_p 0.8 --repetition_penalty 1.2

# åˆ›æ„å†™ä½œï¼ˆæ›´éšæœºï¼‰
--temperature 0.8 --top_p 0.95 --repetition_penalty 1.0

# ä»£ç ç”Ÿæˆï¼ˆå¾ˆç¡®å®šï¼‰
--temperature 0.1 --top_p 0.8 --repetition_penalty 1.1
```

### Q5: è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
- âœ… ä½¿ç”¨ `--ddp` å¤š GPU è®­ç»ƒ
- âœ… ä½¿ç”¨ `--dtype bfloat16` æ··åˆç²¾åº¦
- âœ… è°ƒæ•´ `--num_workers`ï¼ˆé€šå¸¸ 2-4ï¼‰
- âœ… ç¡®ä¿æ•°æ®åœ¨ SSD ä¸Š

## ğŸ“š è¯¦ç»†æ–‡æ¡£

- ğŸ” [æ¨ç†è¯¦ç»†æŒ‡å—](INFERENCE_GUIDE.md)
- ğŸ“– [è®­ç»ƒä½¿ç”¨æ–‡æ¡£](TRAIN_HF_LORA_USAGE.md)
- ğŸ› ï¸ [pos2d å®ç°æ€»ç»“](POS2D_IMPLEMENTATION_SUMMARY.md)
- ğŸ“˜ [ä¸»æ–‡æ¡£](../README.md)

## ğŸ§ª æµ‹è¯•éªŒè¯

```bash
# pos2d å•å…ƒæµ‹è¯•
python scripts/test_pos2d_handling.py

# å®Œæ•´ç³»ç»ŸéªŒè¯
python scripts/validate_inference_setup.py

# å¿«é€Ÿæ¨ç†æµ‹è¯•
bash scripts/test_inference.sh Qwen/Qwen2-0.5B-Instruct out/lora/hf_lora_hf_final.pth 8
```

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å¼€å‘æµç¨‹

```
1. å‡†å¤‡ Parallel æ ¼å¼æ•°æ®ï¼ˆmain + branchesï¼‰
   â†“
2. å°æ¨¡å‹å¿«é€Ÿæµ‹è¯•ï¼ˆQwen2-0.5B, 1 epochï¼‰
   â†“
3. éªŒè¯æ¨ç†æ•ˆæœ
   â†“
4. æ‰©å¤§åˆ°ç”Ÿäº§æ¨¡å‹ï¼ˆQwen2.5-1.5B+, 3 epochsï¼‰
   â†“
5. è°ƒä¼˜è¶…å‚æ•°ï¼ˆlora_rank, rope_2d_ratio, temperatureï¼‰
```

### 2. å‚æ•°é€‰æ‹©

| åœºæ™¯ | lora_rank | rope_2d_ratio | batch_size | epochs |
|------|-----------|---------------|------------|--------|
| å¿«é€Ÿå®éªŒ | 8 | 0.5 | 8 | 1 |
| ç”Ÿäº§è®­ç»ƒ | 16 | 0.5 | 4 | 3 |
| å¤§æ¨¡å‹ | 32 | 0.5 | 1-2 | 3-5 |

### 3. æ•°æ®å‡†å¤‡

```python
# Parallel æ•°æ®æ ¼å¼ç¤ºä¾‹
{
  "main": "ä¸»åˆ†æ”¯å†…å®¹...",
  "branches": [
    "åˆ†æ”¯1å†…å®¹...",
    "åˆ†æ”¯2å†…å®¹...",
    "åˆ†æ”¯3å†…å®¹..."
  ]
}

# æˆ–ç®€åŒ–æ ¼å¼
{
  "text": "ä¸»åˆ†æ”¯å†…å®¹..."
}
```

### 4. Python API è°ƒç”¨

```python
from scripts.inference_hf_lora import load_model_with_lora, generate_text

# åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨å¤„ç† pos2dï¼‰
model, tokenizer, patch_rope = load_model_with_lora(
    base_model="Qwen/Qwen2-0.5B-Instruct",
    lora_path="out/lora/hf_lora_hf_final.pth",
    lora_rank=8,
    rope_2d_ratio=0.5,
)

# ç”Ÿæˆï¼ˆè‡ªåŠ¨å¤„ç† pos2dï¼‰
response = generate_text(model, tokenizer, "ä½ å¥½")
print(response)
```

## ğŸ¯ æ€»ç»“

- âœ… pos2d å·²è‡ªåŠ¨å¤„ç†ï¼Œå¼€ç®±å³ç”¨
- âœ… æ”¯æŒ HuggingFace æ‰€æœ‰ CausalLM æ¨¡å‹
- âœ… LoRA é«˜æ•ˆå¾®è°ƒï¼Œæ˜¾å­˜å‹å¥½
- âœ… 2D RoPE æ”¯æŒå¹¶è¡Œæ•°æ®
- âœ… å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹
- âœ… è¯¦ç»†çš„æ–‡æ¡£å’Œæµ‹è¯•

æœ‰é—®é¢˜è¯·æŸ¥çœ‹ [è¯¦ç»†æ–‡æ¡£](INFERENCE_GUIDE.md) æˆ–æ Issueï¼
