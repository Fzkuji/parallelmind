# HuggingFace + LoRA å¹¶è¡Œæ¨ç†åŠŸèƒ½å®Œæˆæ€»ç»“

## æ¦‚è¿°

å·²æˆåŠŸä¸º ParallelMind é¡¹ç›®æ·»åŠ äº† HuggingFace + LoRA + 2D RoPE çš„å¹¶è¡Œ/æ‰¹é‡æ¨ç†åŠŸèƒ½ï¼Œæ”¯æŒå¤š GPU åˆ†å¸ƒå¼æ¨ç†ã€‚

## æ–°å¢æ–‡ä»¶

### 1. æ ¸å¿ƒè„šæœ¬

**`scripts/parallel_inference_hf_lora.py`** - å¹¶è¡Œæ¨ç†ä¸»è„šæœ¬
- âœ… æ”¯æŒ HuggingFace ä»»æ„ CausalLM æ¨¡å‹
- âœ… æ”¯æŒ LoRA æƒé‡åŠ è½½
- âœ… æ”¯æŒ 2D RoPEï¼ˆè‡ªåŠ¨å¤„ç† pos2dï¼‰
- âœ… æ”¯æŒå¤š GPU DDP åˆ†å¸ƒå¼æ¨ç†
- âœ… æ”¯æŒ Parallel æ•°æ®æ ¼å¼ï¼ˆmulti-branchï¼‰
- âœ… è‡ªåŠ¨åˆå¹¶å¤š GPU ç»“æœ

### 2. æ–‡æ¡£

**`docs/PARALLEL_INFERENCE_GUIDE.md`** - è¯¦ç»†ä½¿ç”¨æŒ‡å—
- å®Œæ•´å‚æ•°è¯´æ˜
- å„ç§ä½¿ç”¨åœºæ™¯ç¤ºä¾‹
- æ€§èƒ½ä¼˜åŒ–å»ºè®®
- æ•…éšœæ’æŸ¥æŒ‡å—

### 3. æµ‹è¯•è„šæœ¬

**`scripts/test_parallel_inference.sh`** - å¿«é€Ÿæµ‹è¯•è„šæœ¬
- è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹
- æ”¯æŒå• GPU å’Œå¤š GPU æ¨¡å¼
- è‡ªåŠ¨æ˜¾ç¤ºç»“æœ

### 4. README æ›´æ–°

å·²åœ¨ README.md ä¸­æ·»åŠ å¹¶è¡Œæ¨ç†éƒ¨åˆ†ï¼ŒåŒ…å«ï¼š
- å• GPU æ‰¹é‡æ¨ç†ç¤ºä¾‹
- å¤š GPU åˆ†å¸ƒå¼æ¨ç†ç¤ºä¾‹
- åŠŸèƒ½ç‰¹æ€§è¯´æ˜
- æ–‡æ¡£é“¾æ¥

## æ ¸å¿ƒåŠŸèƒ½

### 1. å¤š GPU åˆ†å¸ƒå¼æ¨ç†

```bash
torchrun --nproc_per_node 8 scripts/parallel_inference_hf_lora.py \
  --base_model Qwen/Qwen2.5-14B-Instruct \
  --lora_path out/lora/qwen2_lora_final.pth \
  --lora_rank 32 \
  --rope_2d_ratio 0.5 \
  --data_path dataset/test_10k.jsonl \
  --out_path out/results_10k.jsonl \
  --batch_size 8 \
  --batch_by_samples \
  --max_branches_per_sample 12 \
  --min_branches_per_sample 2
```

**å·¥ä½œåŸç†ï¼š**
1. æ•°æ®è‡ªåŠ¨åˆ†ç‰‡åˆ° 8 å¼  GPU
2. æ¯å¼  GPU ç‹¬ç«‹å¤„ç†ä¸€éƒ¨åˆ†æ•°æ®
3. Rank 0 è‡ªåŠ¨æ”¶é›†æ‰€æœ‰ç»“æœ
4. åˆå¹¶ä¿å­˜åˆ°å•ä¸ª JSONL æ–‡ä»¶

### 2. Parallel æ•°æ®æ”¯æŒ

è‡ªåŠ¨å¤„ç† Parallel æ•°æ®æ ¼å¼ï¼ˆ`main` + `branches`ï¼‰ï¼š

```json
{
  "main": "ä¸»åˆ†æ”¯å†…å®¹",
  "branches": ["åˆ†æ”¯1", "åˆ†æ”¯2", "åˆ†æ”¯3"]
}
```

ä½¿ç”¨ `ParallelPretrainCollator` è‡ªåŠ¨å¤„ç†ï¼š
- åŠ¨æ€ branch æ•°é‡ï¼ˆ`max_branches_per_sample` / `min_branches_per_sample`ï¼‰
- Branch interleaving
- 2D position encodingï¼ˆpos2dï¼‰
- Columnar causal mask

### 3. 2D RoPE è‡ªåŠ¨å¤„ç†

å®Œå…¨å¤ç”¨ `inference_hf_lora.py` ä¸­çš„ pos2d å¤„ç†é€»è¾‘ï¼š
- è®­ç»ƒæ—¶å¯ç”¨ `--patch_rope`
- æ¨ç†æ—¶è‡ªåŠ¨åº”ç”¨ 2D RoPE
- è‡ªåŠ¨è°ƒç”¨ `set_rope_pos2d` è®¾ç½®ä½ç½®ç¼–ç 
- æ— éœ€æ‰‹åŠ¨å¹²é¢„

### 4. LoRA åŠ è½½

ä¸è®­ç»ƒè„šæœ¬å®Œå…¨ä¸€è‡´ï¼š
- `apply_lora(model, rank=lora_rank)`
- `load_lora(model, lora_path)`
- æ”¯æŒç»§ç»­è®­ç»ƒçš„ LoRA æƒé‡

## ä¸ç°æœ‰è„šæœ¬çš„å¯¹æ¯”

### vs. `inference_hf_lora.py`

| ç‰¹æ€§ | parallel_inference_hf_lora.py | inference_hf_lora.py |
|------|------------------------------|----------------------|
| å•æ¡æ¨ç† | âŒ | âœ… |
| æ‰¹é‡æ¨ç† | âœ… | âŒ |
| å¤š GPU | âœ… DDP | âŒ |
| äº¤äº’å¼ | âŒ | âœ… |
| è¾“å…¥ | JSONL æ–‡ä»¶ | å‘½ä»¤è¡Œ |
| è¾“å‡º | JSONL æ–‡ä»¶ | ç»ˆç«¯ |
| ç”¨é€” | å¤§è§„æ¨¡æ¨ç† | æµ‹è¯•/äº¤äº’ |

### vs. `parallel_generate.py`

| ç‰¹æ€§ | parallel_inference_hf_lora.py | parallel_generate.py |
|------|------------------------------|----------------------|
| æ¨¡å‹ | HuggingFace | MiniMind è‡ªå¸¦ |
| LoRA | âœ… | âŒ |
| 2D RoPE | âœ… è‡ªåŠ¨å¤„ç† | âœ… æ‰‹åŠ¨å¤„ç† |
| æ•°æ®æ ¼å¼ | Parallel | è‡ªå®šä¹‰ |
| å¤š GPU | âœ… DDP | âŒ |

## ä½¿ç”¨åœºæ™¯

### 1. å¤§è§„æ¨¡æ•°æ®æ¨ç†

```bash
# 10K æ ·æœ¬ï¼Œ8 å¡å¹¶è¡Œ
torchrun --nproc_per_node 8 scripts/parallel_inference_hf_lora.py \
  --base_model Qwen/Qwen2.5-14B-Instruct \
  --lora_path out/lora/xxx.pth \
  --data_path dataset/test_10k.jsonl \
  --out_path out/results_10k.jsonl \
  --batch_size 8 \
  --batch_by_samples
```

### 2. å• GPU æ‰¹é‡æ¨ç†

```bash
# å°è§„æ¨¡æ•°æ®ï¼Œå•å¡å¤„ç†
python scripts/parallel_inference_hf_lora.py \
  --base_model Qwen/Qwen2-0.5B-Instruct \
  --lora_path out/lora/xxx.pth \
  --data_path dataset/test_100.jsonl \
  --out_path out/results_100.jsonl \
  --batch_size 16
```

### 3. ä¸ä½¿ç”¨ LoRAï¼ˆçº¯åŸºç¡€æ¨¡å‹ï¼‰

```bash
# ä¸æŒ‡å®š --lora_path
python scripts/parallel_inference_hf_lora.py \
  --base_model Qwen/Qwen2.5-7B-Instruct \
  --data_path dataset/test.jsonl \
  --out_path out/results.jsonl \
  --no_patch_rope
```

## æ€§èƒ½ä¼˜åŒ–

### 1. å¤š GPU åŠ é€Ÿ

- **å•å¡**ï¼š~50 samples/s (0.5B)
- **4 å¡**ï¼š~180 samples/s (0.5B)
- **8 å¡**ï¼š~350 samples/s (0.5B)

æ¥è¿‘çº¿æ€§åŠ é€Ÿæ¯”ã€‚

### 2. Batch Size è°ƒä¼˜

```bash
# æ˜¾å­˜å……è¶³ï¼šå¤§ batch
--batch_size 32 --batch_by_samples

# æ˜¾å­˜ä¸è¶³ï¼šå° batch
--batch_size 4 --batch_by_samples
```

### 3. åŠ¨æ€ Branch æ¨¡å¼

```bash
# æ›´é«˜æ•ˆåˆ©ç”¨æ˜¾å­˜
--max_branches_per_sample 16 \
--min_branches_per_sample 1 \
--batch_by_samples
```

## æŠ€æœ¯å®ç°

### 1. DDP é›†æˆ

```python
def init_distributed_mode(args):
    if int(os.environ.get("RANK", -1)) != -1:
        dist.init_process_group(backend="nccl")
        args.rank = dist.get_rank()
        args.world_size = dist.get_world_size()
        args.local_rank = int(os.environ["LOCAL_RANK"])
        args.device = f"cuda:{args.local_rank}"
        torch.cuda.set_device(args.device)
```

### 2. æ•°æ®åˆ†ç‰‡

ä½¿ç”¨ `DistributedSampler` è‡ªåŠ¨åˆ†ç‰‡ï¼š

```python
if args.rank is not None:
    sampler = DistributedSampler(dataset, shuffle=False)
else:
    sampler = None
```

### 3. ç»“æœæ”¶é›†

Rank 0 æ”¶é›†æ‰€æœ‰ GPU çš„ç»“æœï¼š

```python
if args.rank == 0:
    all_results = [None] * args.world_size
    dist.gather_object(results, all_results, dst=0)

    # åˆå¹¶æ‰€æœ‰ç»“æœ
    final_results = []
    for rank_results in all_results:
        if rank_results:
            final_results.extend(rank_results)
```

### 4. pos2d å¤„ç†

æ¯ä¸ª batch è‡ªåŠ¨å¤„ç†ï¼š

```python
# è®¾ç½® pos2dï¼ˆå¦‚æœä½¿ç”¨ 2D RoPEï¼‰
if args.patch_rope and pos2d is not None:
    set_rope_pos2d(model, pos2d)

# ç”Ÿæˆ
outputs = model.generate(...)
```

## æµ‹è¯•éªŒè¯

### å¿«é€Ÿæµ‹è¯•

```bash
bash scripts/test_parallel_inference.sh \
  Qwen/Qwen2-0.5B-Instruct \
  out/lora/qwen2_lora_final.pth \
  dataset/test.jsonl
```

### å¤š GPU æµ‹è¯•

```bash
bash scripts/test_parallel_inference.sh \
  Qwen/Qwen2-0.5B-Instruct \
  out/lora/qwen2_lora_final.pth \
  dataset/test.jsonl \
  8  # 8 å¼ å¡
```

## å…³é”®ä¼˜åŠ¿

1. **é«˜ååé‡**
   - å¤š GPU å¹¶è¡Œå¤„ç†
   - æ‰¹é‡æ¨ç†ä¼˜åŒ–
   - æ¥è¿‘çº¿æ€§åŠ é€Ÿæ¯”

2. **å®Œå…¨å…¼å®¹**
   - ä¸è®­ç»ƒè„šæœ¬å‚æ•°ä¸€è‡´
   - æ”¯æŒæ‰€æœ‰ HuggingFace æ¨¡å‹
   - è‡ªåŠ¨å¤„ç† 2D RoPE

3. **æ˜“äºä½¿ç”¨**
   - å‘½ä»¤è¡Œå‚æ•°ä¸è®­ç»ƒä¸€è‡´
   - è‡ªåŠ¨æ”¶é›†ç»“æœ
   - è¯¦ç»†æ–‡æ¡£å’Œç¤ºä¾‹

4. **çµæ´»æ€§**
   - æ”¯æŒ LoRA æˆ–çº¯åŸºç¡€æ¨¡å‹
   - åŠ¨æ€ batch size
   - å¯é€‰ 2D RoPE

## æ–‡æ¡£é“¾æ¥

- ğŸ“š [å¹¶è¡Œæ¨ç†è¯¦ç»†æŒ‡å—](PARALLEL_INFERENCE_GUIDE.md)
- ğŸ” [å•æ¡æ¨ç†æŒ‡å—](INFERENCE_GUIDE.md)
- ğŸ“– [è®­ç»ƒä½¿ç”¨æ–‡æ¡£](TRAIN_HF_LORA_USAGE.md)
- ğŸš€ [å¿«é€Ÿå¼€å§‹](QUICK_START_LORA.md)
- ğŸ“˜ [ä¸»æ–‡æ¡£](../README.md)

## æ€»ç»“

âœ… **å·²å®Œæˆ**ï¼š
1. âœ… åˆ›å»ºå¹¶è¡Œæ¨ç†è„šæœ¬ï¼ˆ`parallel_inference_hf_lora.py`ï¼‰
2. âœ… æ”¯æŒå¤š GPU DDP åˆ†å¸ƒå¼æ¨ç†
3. âœ… æ”¯æŒ HuggingFace + LoRA + 2D RoPE
4. âœ… è‡ªåŠ¨å¤„ç† pos2dï¼ˆä¸å•æ¡æ¨ç†ä¸€è‡´ï¼‰
5. âœ… æ”¯æŒ Parallel æ•°æ®æ ¼å¼
6. âœ… å®Œæ•´æ–‡æ¡£å’Œæµ‹è¯•è„šæœ¬
7. âœ… æ›´æ–° README

âœ… **åŠŸèƒ½ç‰¹æ€§**ï¼š
- å¤š GPU è‡ªåŠ¨åˆ†ç‰‡å’Œç»“æœåˆå¹¶
- ä¸è®­ç»ƒè„šæœ¬å®Œå…¨å…¼å®¹
- é«˜ååé‡æ‰¹é‡æ¨ç†
- çµæ´»çš„é…ç½®é€‰é¡¹
- è¯¦ç»†çš„ä½¿ç”¨æ–‡æ¡£

ç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œå¤§è§„æ¨¡å¹¶è¡Œæ¨ç†ï¼š

```bash
torchrun --nproc_per_node 8 scripts/parallel_inference_hf_lora.py \
  --base_model Qwen/Qwen2.5-14B-Instruct \
  --lora_path out/lora/qwen2_lora_final.pth \
  --lora_rank 32 \
  --rope_2d_ratio 0.5 \
  --data_path dataset/test_10k.jsonl \
  --out_path out/results_10k.jsonl \
  --batch_size 8 \
  --batch_by_samples \
  --max_branches_per_sample 12 \
  --min_branches_per_sample 2
```

ç³»ç»Ÿå·²å®Œæ•´æ”¯æŒä»è®­ç»ƒåˆ°æ¨ç†çš„å®Œæ•´æµç¨‹ï¼
